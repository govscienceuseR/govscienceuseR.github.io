---
title: <center> "The science informing California's Groundwater Sustainability Plans"</center>
subtitle: <center>"A test case using `govscienceuseR` tools" </center>
author: <center>
  - Liza Wood
  - Tyler A. Scott 
toc: true # table of content true
  toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
  number_sections: true 
---

```{r chunk setup, include = F}
knitr::opts_chunk$set(warning = F, message = F)
```

```{r functions, echo= F}
library(showtext)
font_add_google("Roboto mono")
theme_govuser <- function(base_size = 12, font = "mono") {
  theme_minimal(base_size = base_size) +
  theme(text = element_text(colour = "black", 
                            family = font),
       panel.grid = element_line(colour = "#d9d9d9")
      )
}
```

### Introduction  

Aligning scientific supply with policy demand in order to develop evidence-based policy is considered a gold standard by many accounts. The Biden Administration recently reaffirmed that 'It is the policy of [the Biden] Administration to make evidence-based decisions guided by the best available science and data... Scientific findings should never be distorted or influenced by political considerations' (Biden, 2021). This sentiment has been echoed by the calls for use-inspired basic research to deliver innovative solutions in service of social and environmental goals (Stokes 2011; Fuglie & Toole 2014). 

However, there is broad recognition that promoting evidence-based policymaking at face value overlooks challenges related to knowledge politics and the selective use of science. Namely, policymakers' emphasis on science as a neutral arbiter masks the politics of generating and communicating scientific knowledge (Sarewitz 2000). Rather than seeing science as independent, supply and demand of science are politically linked (Sarewitz & Pielke 2007). As such, research to support evidence-based policymaking requires intentional engagement by scientists to not only reduce uncertainty through high-quality research, but also reduce ambiguity through strategic communication and problem framing (Cairney 2016). Emphasis on communication is echoed by literature on boundary spanners that can broker between the scientific and policy communities (Guston 2001).  

<!--- Qualitative case descriptions have described the recursive science-policy relationship for environmental issues such as acid rain, ozone repletion, and climate change (Pielke & Betsill 1997; Sarewitz 2000)--->

Recognizing the complex interplay of science and policymaking, what methods can we use to better understand these dynamics? Through surveys and interviews of public servants, studies find wide variation in how policy makers interact with scientific information (Newman et al. 2016), and highlight that internal governmental reports and personal networks are among the most popular sources of information (Piczak et al. 2022). These findings have been supported and detailed through a handful of bibliometric analyses of policy documents. Desmaris and Herd (2014) evaluated 102 Regulatory Impact Assessments, confirming a majority of governmental citations and describing the prevalence of disciplines like economics, environmental science, and public health from high-impact journals across 1,378 scholarly citations. Koontz (2022) finds similar trends when analyzing 12 salmon recovery plans, categorizing 1,104 references from various sources (academic, governmental, organizational, etc.) and expanding on how science is used to support causal arguments.  

The insights from bibliometric analyses of policy documents lay an important foundation for systematically quantifying the evidence used to support policy. However, manually extracting and indexing references can be a tedious task. Unlike 'science of science' research, where bibliometric methods are made easier through publishing norms of citations and indexing (e.g. DOIs, ORCIds, etc.), science of policy research is less systematic. Policy documents do not always follow consistent formatting or citations norms, and references outside of non-scholarly articles (which are the majority of references) are not systematically indexed. However, developments in computational tools for reading and analyzing large texts create an opportunity for automating some of the more tedious phases of bibliometry on policy documents. This paper presents a set of tools for aiding bibliometic analysis of policy documents, `govscienceuseR`, which we hope will support policy process researchers in quantifying and unpacking the complex interplay of science and policymaking.  

### Method  

The `govscienceuseR` tools provide a computational approach for indexing and summarizing references from policy documents. This is vignette walking through the steps of the R packages in the `govscienceuseR` tool set: `referenceExtract`, `referenceClassify`, `indexBuild`, and `referenceSearch`. Together, these four packages allow researchers to go from PDF documents to a data frame of indexed citations in only a handful of steps in the R coding language. The goal of these tools is to allow researchers working with various types of policy documents to analyze citations using a systematic and reproducible approach.  

### The case: Groundwater Sustainability Plans    

California's Sustainable Groundwater Management Act (SGMA) of 2014 sets out a framework for local Groundwater Sustainability Agencies to work towards sustainable management of their groundwater basins. Agencies are required to outline their strategies in Groundwater Sustainability Plans (GSPs) -- documents reviewed by the Department of Water Resources (DWR) to ensure that Agency strategies comply with SGMA, the GSP regulations, and achieve groundwater sustainability for the basin (DWR no date). Among the criteria for plan evaluation, the California Code specifies: "When evaluating whether a Plan is likely to achieve the sustainability goal for the basin, the Department shall consider… Whether the assumptions, criteria, findings, and objectives, including the sustainability goal… are reasonable and supported by the best available information and best available science" (23 CCR § 355.4).  

There is a clear mandate that GSP documents use scientific evidence to support their strategies for groundwater sustainability, but defining and measuring sustainability is no simple feat (Kuhlman & Farrington 2010). DWR sets out six sustainability indicators, whereby the occurrence of any indicator would be an undesirable result: lowering groundwater levels, reduction of storage, seawater intrusion, degraded quality, land subsidence, and surface water depletion. However, it is the Agencies that set measurable objectives and minimum thresholds for these indicators by considering all 'beneficial users and uses' for their basin (Austin, 2019). Beneficial uses include 'domestic, municipal, agricultural, and industrial supply; power generation; recreation; aesthetic enjoyment; navigation; and preservation and enhancement of fish, wildlife, and other aquatic resources or preserves' (California Water Plan glossary 2018). 

Facing the challenge of managing groundwater resources across a wide array of users and interests, we ask: **How is science used to inform the multiple dimensions of groundwater sustainability?** Analyses of the SGMA collaborative process and GSP drafts have already shown that achieving groundwater sustainability while supporting _all_ beneficial uses is unlikely. 
* Some beneficial uses are preferenced over others: domestic well depths above minimum threshold (Bostic et al. 2020. Sustainable for whom?)  
* Representation challenges (Dobbin)

To build on this existing research, we turn to the scientific evidence used in GSPs to tell us something about how agencies are defining sustainability. (and as a result, preferencing some groundwater users over others?). We trial the govscienceuseR tool kit on 114 GSPs, published between X and X year...


### Getting started   

#### Installations  

To begin, download all four packages from the [govscienceuseR GitHub page](https://github.com/govscienceuseR):  

```{r package download, warning = F, message = F, results = F}
devtools::install_github("govscienceuseR/referenceExtract")
devtools::install_github("govscienceuseR/referenceClassify")
devtools::install_github("govscienceuseR/referenceBuild")
devtools::install_github("govscienceuseR/citationSearch")
```

Then load these and dependent packages into library. [**Note: All of the dependent packages required should be imported when we load in these packages, but they currently aren't -- still strugglin with package imports when function-building**]. You may be prompted to install some of the dependent packages if you do not already have them, such as `keras`.  

```{r package setup, warning = F, message = F, results = F}
library(referenceExtract)
library(referenceClassify)
library(referenceBuild)
library(citationSearch)
packs = c('data.table', 'dplyr', 'stringr', 'keras', 'tensorflow',
          'tidyr', 'purrr')
sapply(packs, require, character.only = T)
```

Outside of R, the `referenceSearch` package also requires installation and running of  [Solr](https://solr.apache.org). This software should be downloaded to a known location on your computer.    

<br>

#### Data    

The documents of interest for this paper are California's Groundwater Sustainability Plans. These documents are available publicly to download from [Box](https://ucdavis.box.com/s/9m3dogbibkrwy01pg4sdwouhzl8i6sef). To follow along with this tutorial, download the files to a file that will be considered your document directory. My document directory is specified below. There are `r length(list.files("~/Box/reference_classifier/documents_gsp/"))` plans which total over 160,000 pages.  

```{r set doc dir}
doc_directory <- "~/Box/reference_classifier/documents_gsp/"
```

```{r count pages, eval = F, echo = F}
pdfinfo <- lapply(list.files(doc_directory, full.names = T),
               pdftools::pdf_info)
pdfs <- data.frame(do.call('rbind', pdfinfo))
pages <- sum(as.numeric(pdfs$pages))
pages/nrow(pdfs)
```

Using these GSP documents we'll be walking through the following steps of the process:

```{r workflow image, echo=FALSE, out.width='75%', fig.align = "center"}
knitr::include_graphics('img/workflow.png')
```

### 1. `referenceExtract`  

The `referenceExtract` package from govscienceuseR is designed to take unstructured PDF documents, feed them through the [anystyle.io](https://anystyle.io/) citation extraction software, and return tagged citation data in a tabular format.   

<br>  

#### `reference_extract()`   

The first step of extracting references is to input PDF documents into the `reference_extract()` function. This function reads in every PDF in the document directory (doc_dir), and runs them through anystyle.io. Anystyle extracts probable citations and exports them to the reference directory (ref_dir) as JSON files. Depending on the number and size of files, this can take some time. For example, these 114 documents took over an hour to extract.  

```{r set ref dir covert, echo = F}
ref_directory <- "data/ref_dir"
```

```{r reference extract, eval = F}
# Extract probable citations from PDF documents and convert them to .json files
ref_directory <- "data/ref_dir"
reference_extract(doc_dir = doc_directory, 
                 ref_dir = ref_directory, 
                 layout = "none")
```

<br>  

#### `reference_compile()`   

Next, the `reference_compile()` function transforms the JSON files into tabular data and compiles them all in one data table, adding the file name as an identifier.  

```{r reference compile}
# Compile json files into a singular tabular data table
dt <- reference_compile(ref_directory)
```

After these first two steps we can take a look at our probable citations according to the Anystyle software. Initially, there are `r nrow(dt)` probable citations across these `r length(list.files(ref_directory))` documents.  

```{r show compiled data, echo = F}
DT::datatable(head(select(dt, author, date, title, `container-title`),
                   n = 100), rownames = F)
```

It is noticeable above that the data provided by Anystyle has its challenges for further analysis. The first is a related to data structure: authors are nested into matrices, some rows (such as the date) have multiple listed values, etc., all of which make the data hard to analyze. For example, here is what seems to be two probable citations combined into one observation:  

```{r challenge example 1}
unnest(dt[204, c(2,3)])
```

The second challenge is related to quality: many of the probable citations are not sensible. The Anystyle software seems sensitive to multiple formats like numbers and short-form sentences, such as addresses, resulting in false positive identification of references. For example, here is an address listed as a probable citation from our data:  
```{r challenge example 2}
unlist(dt[148, c(1,3,5)])
```

<br>  

#### `reference_clean()`  

To try to address the challenges in these probable citations, the `reference_clean()` function goes through a series of steps. For each column the function unlists the data and filters out unlikely candidates for that column. For instance, if a number listed in the date column does not match any reasonable date format or expectation, it is removed. If a string in the URL column actually resembles a DOI, it is moved to that column. And so on. Furthermore, if there seem to be multiple citations listed in one row that can be broken apart in parallel across all of the columns, we unnest these rows. (Note: depending on the size of the files this function may take some time. Cleaning these `r nrow(dt)` citations takes about 18 minutes).  

```{r reference clean, eval = F}
# Unnests list structures into tabular data and filters out low probability refs
dt <- reference_clean(dt)
```

```{r load ref clean dt covert, echo = F}
#saveRDS(cleaned_dt, "data/dt_post_refclean.RDS")
dt <- readRDS("data/dt_post_refclean.RDS")
```

This cleaning process has changed probable citations now a bit. Some of the probable citations have been unnested (and therefore expanded) while others have been removed, leaving us with `r nrow(dt)` probable citations. Examples of these probable citations are below:   

```{r example of cleaned, echo = F}
DT::datatable(head(select(dt, author, year, title, container, publisher), n = 100), rownames = F)
```


### `referenceClassify`  

The `referenceClassify` package is designed to take a data frame of tabular, tagged citation data (author, year, container, publisher, doi, etc), look for exact matches between those tags and various high-level indices (mainly journal and agency names), and begin to classify probable citations into these high-level categories. These indices are 1:) an index of academic journals from the [`sjr` package](https://rdrr.io/github/ikashnitsky/sjrdata/man/sjr_journals.html) relying on the [Scimago database](https://www.scimagojr.com/), 2) an index of academic conference papers/proceedings also from the Scimago database, 3) and an index of US state and federal agencies, curated by the package authors. All three of these indices are built into this package and can be accessed with `data(scimago.j)` for journals, `data(scimago.c)` for conferences, and `data(agencies)` for agencies.  

<!---
#### `_match()` functions  

Based on our initial cleaning of the GSP data in the previous section, we can see which of these probable citations are exact matches to three different indices: an index of academic journals from the Scimago database, an index of academic conference papers/proceedings also from the Scimago database, and an index of US state and federal agencies, curated by the package authors. We can look for matches using the `journal_match()`, `conference_match()` and `agency_match()` functions.

```{r}
table(journal_match(dt$container))
```

```{r}
table(conference_match(dt$container))
```

```{r}
table(agency_match(dt$container, dt$author))
```
--->
<br>  

#### `prepared_by()` and `journal_disambig()`  

[**Note: Should these be moved to the cleaning function, or to the other package?**] 

Before classifying our probable references, we can further clean and refine these potential citations with the `prepared_by()` function, which removes commonly-seen lead-ins to references ('prepared for/by', etc.) to improve exact matching.  

```{r prepared by}
dt <- prepared_by(dt, x = 'container', y = 'author', z = 'publisher')
```

Additionally, we disambiguate the journals with the `journal_disambig()` function, which references indices of common journal abbreviations and through manual cleaning of journals referenced in transportation documents.  

```{r journal disam}
dt$container <- journal_disambig(dt$container)
```
<br>  

#### `regex_classify()`  

Now with probable references as 'clean' as possible, we use regular expressions to classify our data based on exact matches using the `regex_classify()` function. This function does two things. First it looks across all of the columns for exact matches to our indices, and if there is an exact match, it pulls out that value into a 'input' column. If there is no exact match, the value in the input column will be selected in the following order of preference: doi, container, publisher, title, author. Second, based on the matches the function will assign the potential citation into one of four classes: journal, agency, conference, or none. If none of the potential citations' data is an exact match to any of the indices, the classification is NA. 

```{r regex classify}
# Extract most descriptive 'input' and look for exact match to index 
dt <- regex_classify(dt, 'container')
```

```{r exact match agencies, echo = F, eval = F}
data("agencies")
x = dt$training_input[dt$class == "agency" & !is.na(dt$class)]
abbr <- agencies$Abbr[agencies$Abbr != ""]
y = c(agencies$Agency, abbr)

dt_agencies <- filter(dt, class == "agency" & !is.na(class)) %>% 
  select(ID, training_input)
dt_agencies$agency = NA
t1 <- Sys.time()
for(i in 1:length(x)){
  for(j in 1:length(y)){
    match <- str_extract(x[i], y[j])
  if(!is.na(match)){
  dt_agencies$agency[i] <- match
  }
  }
    if(!is.na(match)){
      next
  }
}
t2 <- Sys.time()
t2-t1 

dt <- left_join(dt, dt_agencies)
dt$agency <- ifelse(dt$agency == "Geological Survey", 
                    "US Geological Survey", dt$agency)
dt$agency <- ifelse(dt$agency == "Army", 
                    "US Army", dt$agency)
dt$agency <- ifelse(dt$agency == "Army Corps of Engineers", 
                    "US Army Corps of Engineers", dt$agency)
dt$agency <- ifelse(dt$agency == "Bureau of Reclamation", 
                    "US Bureau of Reclamation", dt$agency)
dt$agency <- ifelse(dt$agency == "Census Bureau", 
                    "US Census Bureau", dt$agency)
dt$agency <- ifelse(dt$agency == "Department of Agriculture", 
                    "US Department of Agriculture", dt$agency)
dt$agency <- ifelse(dt$agency == "Department of Health and Human Services", 
                    "US Department of Health and Human Services", dt$agency)
dt$agency <- ifelse(dt$agency == "Department of the Interior", 
                    "US Department of the Interior", dt$agency)
dt$agency <- ifelse(dt$agency == "Environmental Protection Agency", 
                    "US Environmental Protection Agency", dt$agency)
dt$agency <- ifelse(dt$agency == "Fish and Wildlife Service", 
                    "US Fish and Wildlife Service", dt$agency)
dt$agency <- ifelse(dt$agency == "Forest Service", 
                    "US Forest Service", dt$agency)
saveRDS(dt, 'data/dt_withagencies.rds')
```

```{r, echo = F}
dt <- readRDS('data/dt_withagencies.rds')
```

Based on these classifications, we can see the counts of exact matches, and which ones have not been classified.   

```{r, echo = F, fig.align = 'center', results = 'asis'}
table <- data.frame("n" = c(table(dt$class), table(is.na(dt$class))[2]))
table$class <- c("Agency", "Conference", "Journal", "Not a citation", "Unclassified")
kableExtra::kable(pivot_wider(table, names_from = class, 
                              values_from = n), booktabs = T, format = "html")
```
<br>  

#### `keras_classify()`  

Next we want to classify the probable references that are not exact matches to any of our indices. To do this, we use `keras_classify()` input our probable into a neural network trained to predict the reference class. To train this model, we used data from Environmental Impact Statements, classified through both manual classification and the semi-automated regex classification explained above.  

```{r keras, eval = F}
# Use the descriptive 'input' to probabilistically identify the reference class
## Note: Need to set this wd right now because I can't actually get the model object to save within the package itself
setwd("~/Documents/Davis/R-Projects/referenceClassify/")
# Something is wrong with the auto_input
predictions <- keras_classify(dt, probability = .85, 
                              'container', auto_input = F,
                              'training_input') 
dt <- cbind(dt, select(predictions, predict_class))
```

```{r load post keras dt covert, echo = F}
#saveRDS(dt, "data/dt_post_keras.RDS")
dt <- readRDS("data/dt_post_keras.RDS")
```

Because we ran this model on the whole data frame, we can compare our regex classification with the Keras classifications to get a sense of the model performance.  

```{r, echo = F, fig.align='center'}
dt <- dt %>% 
  mutate(class = case_when(
           class == "remove_row" ~ "delete",
           T ~ class),
         method = case_when(
           is.na(class) ~ "keras",
           T ~ "regex"),
         method_comparison = case_when(
           method == "regex" & class == predict_class ~ "match",
           method == "regex" & predict_class == "unsure" ~ "unsure",
           method == "regex" & class != predict_class ~ "incorrect",
           T ~ NA_character_))

table <- data.frame(table(dt$method_comparison)) %>% select(-Var1)
table$class <- c("Incorrect", "Match", "Classified as unsure")
kableExtra::kable(pivot_wider(table, names_from = class, 
                              values_from = Freq), booktabs = T)
```

These results, altogether, suggest that the Keras model is `r 100*round(table(dt$method_comparison)[[2]]/sum(table(dt$method_comparison)),2)`% accurate in its prediction of the citations we are able to do exact matching for. Now, let's unify the classification columns and have a look at the total for each estimated grouping.   

```{r, echo = F, fig.align='center'}
dt$class <- ifelse(is.na(dt$class), dt$predict_class, dt$class)
table <- data.frame(table(dt$class)) %>% select(-Var1)
table$class <- c("Agency", "Conference", "Not a citation",
                 "Journal", "Unsure")
kableExtra::kable(pivot_wider(table, names_from = class, values_from = Freq), 
                  booktabs = T)
```

And we'll tidy up these data by filtering out the citations classificied to 'delete' (the false positives) in preparation for our indexing in the next step.   
```{r}
dt <- dt %>% 
  select(-c(predict_class, method_comparison)) %>% 
  filter(class != "delete")
```
<br>  

#### Interim check in: Journals and agencies from exact matching   

So far, we've identified exact matches to our high-level indices (journal, agency, conference) and then used machine learning to classify citations that are not exact matches so that we can begin to figure out their source. At this interim stage, we can take a look at the high-level classifications across citations, and the journals and agencies for which we've found exact matches.  

First, let's take a look at the high-level matches. A reference list was required by DWR for GSPs, and indeed we see that all `r length(unique(dt$File[dt$class %in% c("journal", "agency")]))` of the documents have references that we can identify as either a scholarly journal or agency. Based on our tool's probabilistic tags, it looks like the number of references ranges from 1 to 128, with 24 being the average number of classified references. That average differs between references classes, whereby the average number of journal references is 16 and the average number of agency references is twice that, at 32. In total, there are 1,865 journal references and 3,675 agency references.  

```{r summaries of references, echo = F, results = F}
by_doc <- dt %>% 
  filter(class %in% c("journal", "agency")) %>% 
  group_by(File, class) %>% 
  count() %>% 
  ungroup() 

summary(by_doc$n)

by_doc %>% 
  group_by(class) %>% 
  summarise(mean = mean(n),
            min = min(n),
            max = max(n))

dt %>% 
  filter(class %in% c("journal", "agency")) %>% 
  group_by(class) %>% 
  count()
```

Of these references, we display the top 15 references journals and agencies identified by our exact matching strategies. At a glance, we see a strong representation of geological, hydrological, and agricultural science...

```{r wrangle data for plots, echo = F}
data("scimago.j")
# Get metadata to be a summarize of 
journal_metadata <- data.frame(sjrdata::sjr_journals) %>% 
  group_by(sourceid, title) %>% 
  summarize(sjr_avg = mean(sjr, na.rm = T)) %>% 
  right_join(select(data.frame(sjrdata::sjr_journals), sourceid, title, categories)) %>% 
  separate(categories, into = paste0("cat", 1:15), sep = ";") %>% 
  pivot_longer(cols = cat1:cat15, 
               names_to = "number",
               values_to = "cat") %>% 
  filter(!is.na(cat)) %>% 
  mutate(cat = trimws(str_remove_all(cat, '\\(Q\\d\\)'))) %>% 
  mutate(cat = trimws(str_remove_all(tolower(cat),
                                     '\\(miscellaneous\\)|,|-'))) %>% 
  select(-number) %>% 
  unique() %>% 
  rename("journal_title" = "title") %>% 
  group_by(sourceid, journal_title, sjr_avg) %>% 
  mutate(number = paste0('cat', 1:n())) %>% 
  pivot_wider(id_cols = c(sourceid, journal_title, sjr_avg),
              names_from = "number", values_from = "cat") %>% 
  unique()

dt_agencies <- dt %>% 
  filter(class == "agency") 

dt_journals <- dt %>% 
  filter(class == "journal" & !is.na(container)) %>% 
  left_join(scimago.j, by = c("container" = "title")) %>% 
  left_join(journal_metadata, by = "sourceid") %>% 
  filter(!is.na(sourceid))

dt_journals$container[dt_journals$container == "Proceedings of the National Academy of Sciences of the United States of America"] <- "PNAS"

```

```{r, echo = F}
library(ggplot2)
p_jour <- dt_journals %>% 
  group_by(container) %>% 
  count() %>% 
  arrange(n) %>% 
  filter(n > 13) %>% 
  mutate(container = factor(container, levels = .$container)) %>% 
  ggplot(aes(x = container, y = n)) +
  geom_col(fill = "#03989e") +
  coord_flip() +
  theme_govuser(base_size = 10)

p_agency <- dt_agencies %>% 
  group_by(agency) %>% 
  filter(!is.na(agency)) %>% 
  count() %>% 
  arrange(n) %>% 
  filter(n > 15) %>% 
  mutate(agency = factor(agency, levels = .$agency)) %>% 
  ggplot(aes(x = agency, y = n)) +
  geom_col(fill = "#03989e") +
  coord_flip() +
  theme_govuser(base_size = 10)
```


```{r, fig.align='center', fig.width=12, fig.height=3}
cowplot::plot_grid(p_jour, p_agency, rel_widths = c(1, 1.05))
```

Regarding the impact factor of academic references...

```{r, fig.align='center', fig.width=4, fig.height=3}
dt_journals %>% 
  group_by(journal_title, sjr_avg) %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(x = sjr_avg, y = n)) +
  geom_point() +
  geom_smooth(method = 'lm', color = "#03989e") +
  labs(x = 'Journal impact factor', y = 'Number of citations') +
  theme_govuser(base_size = 10)
```


### `indexBuild`  

Now that we have a general sense of what kind of reference types are represented within the probable citations, the next step is to try to index these reference exactly. The `indexBuild` package is designed to query the [openAlex API](https://docs.openalex.org/), an open access catalog of research, in order to build a personalized catalog relevant to the field of research. This step is optional for the `govscienceuseR` workflow, as the tool kit provides a default index with the top X research categories over the past X years [**Note: need to build some default index**]. However, building an index for a given research project will likely increase the matching abilities of the tool.  

#### Identify index concepts  

To build an index, we first need to define the scope. openAlex allows indices to be built based on five different types of entities: authors, institutions, venues, concepts, and works. We choose to build our index based on concepts, as this casts the widest and most inclusive net. We use the known journal names we already have in our data to define the concepts that we want in build our index on. To identify these concepts we use journals' disciplinary category metadata from the Scimago database.   

```{r}
themes <- dt_journals %>% 
  select(sourceid, journal_title, cat1:cat13) %>% 
  pivot_longer(cols = cat1:cat13, 
               names_to = "number",
               values_to = "cat") %>% 
  filter(!is.na(cat)) %>% 
  group_by(cat) %>% 
  count() %>% 
  arrange(n) %>% 
  mutate(cat = factor(cat, levels = .$cat))
```

Across the journals identified in our references there are `r nrow(themes)` disciplinary categories and the top categories are displayed below.  

```{r, echo = F}
p_theme <- themes %>% 
  filter(n > 48) %>% 
  ggplot(aes(x = cat, y = n)) +
  geom_col(fill = "#03989e") +
  coord_flip() +
  labs(y = "Count", x = "", title = "Top journal themes") +
  theme_govuser(base_size = 10)
```

```{r, fig.height = 3, fig.width = 5, fig.align='center', echo = F,}
p_theme
```

```{r}
keywords <- tidytext::unnest_tokens(themes, word, cat) %>% 
  dplyr::filter(!word %in% tidytext::stop_words$word) %>% 
  mutate(word = textstem::lemmatize_words(word)) %>% 
  unique()
```

Let's use these themes and keywords to inform our index creation. I build in a quick bit of sleep time for querying the concepts from openAlex, then apply this functions across all of the themes and keywords existing in the data. (Note: This querying of over 200 concepts takes ~5 minutes.)
<br>  

#### `queryConcepts()`  

```{r, eval = F}
query_slowly <- function(x){
  index <- queryConcepts(concept_string = x, 
                         per_page = 50)
  return(index)
  Sys.sleep(1)
}
index <- lapply(c(themes$cat, keywords$word), query_slowly)
# For whatever reason there was no description column for 7
index[[12]]$description <- ""
indexdf <- do.call('rbind', index) %>% unique()
```

Now we have a number of resources representing the concepts from our existing data. Here is a random look at some of them.

```{r, echo = F}
#saveRDS(indexdf, "data/indexdf.rds")
indexdf <- readRDS("data/indexdf.rds")
```


```{r, echo = F}
DT::datatable(sample_n(indexdf, 100)[-c(3,5)], rownames = F)
```

What we end up is a list of concepts, related to which are different 'works', or specific citations. We can extract the works from those concepts below. I currently break this into two extraction rounds, small and large, because they need to be handled differently. This extraction c

<br>  

#### `extractWorks()`  

```{r, eval = F}
sapply(indexdf$id, FUN = function(x){
  extractWorks(mailto = "belwood@ucdavis.edu",
             concept_page = x,
             dest_file = paste0("~/Box/govscienceuseR/openalex_index_gsp/", 
                              stringr::str_extract(x,'[A-Za-z0-9]+$'),
                              "_2000_2020", ".json.gz"),
             per_page = 200, # must be between 1 and 200
             to_date = 2022,
             from_date = 2000,
             sleep_time = 0.5)

})
```


```{r, eval = F, echo = F}
smallworks <- filter(indexdf, works_count < 1000000)

# I did it this way so that if it breaks due to internet connection failure I can see where it leaves off
for(i in 125:nrow(smallworks)){
sapply(smallworks$id[i], FUN = function(x){
  extractWorks(mailto = "belwood@ucdavis.edu",
             concept_page = x,
             dest_file = paste0("~/Box/govscienceuseR/openalex_index_gsp/", 
                              stringr::str_extract(x,'[A-Za-z0-9]+$'),
                              "_2000_2020", ".json.gz"),
             per_page = 200, # must be between 1 and 200
             to_date = 2022,
             from_date = 2000,
             sleep_time = 0.5)

})
}
```

```{r, eval = F, echo = F}
largeworks <- filter(indexdf, works_count >= 1000000)

for(i in 1:nrow(largeworks)){
  for(j in 2000:2022){
sapply(largeworks$id[i], FUN = function(x){
  extractWorks(mailto = "belwood@ucdavis.edu",
             concept_page = x,
             dest_file = paste0("~/Box/govscienceuseR/openalex_index_gsp/", 
                              stringr::str_extract(x,'[A-Za-z0-9]+$'),
                              "_", j, ".json.gz"),
             per_page = 100,
             to_date = j,
             from_date = j,
             sleep_time = 0.5)

})
}
}

```

<br>   

#### `works2dt()`  

Here, extractions of 117 concepts resulted in over 3 million works. This conversion from json concepts to works took 12.5 hours.
```{r compile records, eval = F}
jsons <- list.files("~/Box/govscienceuseR/openalex_index_gsp/", full.names = T)
# Anything that is 23 is empty
jsons <- jsons[file.size(jsons) > 23]
t1 <- Sys.time()
records <- lapply(jsons, works2dt)
recordsdf <- do.call("rbind", records)
t2 <- Sys.time()
t2-t1
```

```{r loads records df covert, echo = F}
#saveRDS(recordsdf, "~/Box/govscienceuseR/records.rds")
recordsdf <- readRDS("~/Box/govscienceuseR/records.rds")
```


We've generated quite a wide database, including [draft number based on only a fraction of the concepts of interest: `r nrow(recordsdf)`] records in our index. However, this likely leaves out edge cases, and we are developing ways to develop broader indices.


### 4. `referenceSearch()`  

Now we have our probable citations and their groupings, we have an index to match them to, and now we want to probabilistically match them. 

Let's isolate our probable citations that would likely map onto our index of academic reference. 
```{r}
dt_solr <- dt %>% 
  filter(class == "journal") %>% 
  select(title, author, year, publisher, container,  
         doi) %>% 
  rename("journal_title" = container,
         "authors" = author) %>% 
  mutate(year = as.numeric(year))

queries <- create_queries(dt_solr)
```

This is where you'll need your Solr instance running
In your command line/terminal, navigate to your solr download, then start a cloud instance by ..

```
path/to/solr.9.0.1/bin/solr start -c
```

```{bash end solr 1, echo = F, eval = F}
~/Applications/solr-9.1.0/bin/solr stop
```

```{bash start solr}
~/Applications/solr-9.1.0/bin/solr start -c
```



```{r set up records}
#records <- "~/Box/govscienceuseR/openalex_index_gsp/"
colnames(recordsdf)[c(1,2,3,5,7,8,10,12)] <- c("source", "title", "doi", 
                                               "year", "miscid",
                                           "journal_title",
                                           "publisher", "authors")
```

```{r index records, eval = F}
index_records(recordsdf, collection_name = "gsp_index", overwrite = T)
```

This does not run, I get " Error: 400 - undefined field year "
```{r query index one, eval = F}
results <- search_collection(q = queries[3], 
			                       collection_name = "WOS_demo", 
			                       topn = 3)
```

Waiting to run then, then

```{r query index large, eval = F}
results = list()
count = 1
for (q in queries[1:10]) {
  res = search_collection(q, collection_name = "gsp_index")
  res$id = count
  res$q = q
  results[[count]] = res
  count = count + 1
}
results_df = do.call(dplyr::bind_rows, results)
```

```{bash end solr 2}
~/Applications/solr-9.1.0/bin/solr stop
```
